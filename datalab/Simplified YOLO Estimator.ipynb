{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of indic2019",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18TyZ2gdj1M0",
        "colab_type": "text"
      },
      "source": [
        "Import statement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOHQBK7L2xGL",
        "colab_type": "code",
        "outputId": "f7131ba2-6a77-48f9-bbd7-0f5050ca41df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tarfile\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import IPython.display as display\n",
        "import tensorboardcolab as tbc\n",
        "tbc = tbc.TensorBoardColab()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://3f91bf49.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEnpRvIIj8Bf",
        "colab_type": "text"
      },
      "source": [
        "Write in tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YODBqo73ST5",
        "colab_type": "code",
        "outputId": "bbec72b5-f466-4ea2-ddb6-6877948745cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "writer = tbc.get_writer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0614 19:10:20.196803 140657897322368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:49: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfs1-_8BkBCH",
        "colab_type": "text"
      },
      "source": [
        "Defining the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY0PLFYt7kY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 10\n",
        "nb_boxes=1\n",
        "grid_w=2\n",
        "grid_h=2\n",
        "cell_w=14\n",
        "cell_h=14\n",
        "img_w=28\n",
        "img_h=28\n",
        "img_channels = 1\n",
        "input_shape = (None, img_w, img_h, img_channels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaAb8BnjkGAX",
        "colab_type": "text"
      },
      "source": [
        "Extract function takes data record and return images and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALfgwMiBwrcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_fn(data_record):\n",
        "    features = {\n",
        "      'image': tf.FixedLenFeature([], tf.string),\n",
        "      'label': tf.FixedLenFeature([5], tf.float32)\n",
        "    }\n",
        "    data = tf.parse_single_example(data_record, features)\n",
        "    img1 = tf.decode_raw(data['image'], tf.float32)\n",
        "    img1 = tf.reshape(img1, (img_w, img_h, img_channels)) #reshape to 28 x 28 x 1\n",
        "    img1 = tf.image.per_image_standardization(img1) #Standardizing\n",
        "    return img1, data['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5olAO8qcwyk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files_pattern = '/gdrive/My Drive/indic2019/TFRecords/*.tfrecord' #file pattern for tf record\n",
        "test_pattern = '/gdrive/My Drive/indic2019/Test/*.tfrecord' #file pattern for testing it on the test pattern\n",
        "test_image = '/gdrive/My Drive/indic2019/images/10_10.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_dayQslkr_S",
        "colab_type": "text"
      },
      "source": [
        "<h3> Input function</h3>\n",
        "  <ul>\n",
        "    <li>Input parameters are files pattern(for taking data from the input), batch_size and mode(predict,eval and train)</li>\n",
        "  <li>Returns features and label in case of eval and train and features in case of predict</li>\n",
        "    </ul>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRK9pLE237aL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn(files_pattern,batch_size, mode):\n",
        "  print(batch_size)\n",
        "  files = tf.data.Dataset.list_files(files_pattern, shuffle=True)\n",
        "  dataset = files.apply(tf.contrib.data.parallel_interleave( lambda filename: tf.data.TFRecordDataset(filename), cycle_length=1)) \n",
        "  #T parallel_interleave-HIS FUNCTION IS DEPRECATED. (not exactly know)\n",
        "    \n",
        "  #three variables for three mode\n",
        "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "  is_eval = (mode == tf.estimator.ModeKeys.EVAL)\n",
        "  is_predict = (mode== tf.estimator.ModeKeys.PREDICT)\n",
        "  \n",
        "  buffer_size = batch_size * 2 + 1\n",
        "  dataset = dataset.shuffle(buffer_size=buffer_size)\n",
        "\n",
        "  # Transformation\n",
        "  dataset = dataset.map(extract_fn)\n",
        "  \n",
        "  if is_training or is_predict:\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(2 * batch_size)\n",
        "    \n",
        "  if is_eval:\n",
        "    buffer_size = batch_size * 10\n",
        "    dataset = dataset.shuffle(buffer_size=buffer_size)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(10 * batch_size)\n",
        "\n",
        "  image, label = dataset.make_one_shot_iterator().get_next()\n",
        "  features = {'images': image}\n",
        "  \n",
        "  if is_training or is_eval:\n",
        "    return features, label\n",
        "  \n",
        "  if is_predict:\n",
        "    return features "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2VKVF0MvNR0",
        "colab_type": "text"
      },
      "source": [
        "<h3> Custom loss </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkD41DymzZxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def custom_loss(labels,logits):\n",
        "  \n",
        "  print(\"Printing labels\")\n",
        "  print(labels)\n",
        "  \n",
        "  true_confidence = labels[:,0]\n",
        "  true_x=labels[:,1]\n",
        "  true_y=labels[:,2]\n",
        "  true_w=labels[:,3]\n",
        "  true_h=labels[:,4]\n",
        "\n",
        "  predict_confidence=logits[:,0]\n",
        "  predict_x=logits[:,1]\n",
        "  predict_y=logits[:,2]\n",
        "  predict_w=logits[:,3]\n",
        "  predict_h=logits[:,4]\t\n",
        "\n",
        "  xy_loss= K.square(true_x-predict_x) + K.square(true_y-predict_y)\n",
        "  wh_loss= K.square(K.sqrt(true_w)-K.sqrt(predict_w))+ K.square(K.sqrt(true_h)-K.sqrt(predict_h))\n",
        "\n",
        "  con_loss=K.square(true_confidence-predict_confidence)\n",
        "\n",
        "  loss= xy_loss + wh_loss + con_loss\n",
        "  return tf.math.reduce_mean(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL28NPPyvm26",
        "colab_type": "text"
      },
      "source": [
        "<h3>Bounding Box</h3>\n",
        "Returns coordinates of bounding box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVmc-_5tMY60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Shape of y : conf x y w h\n",
        "\n",
        "def convert_to_coord(y):\n",
        "  coord= []\n",
        "  bb_box_width = y[:,3] * img_w\n",
        "  bb_box_height = y[:,4] * img_h\n",
        "  center_x = y[:,1] * img_w\n",
        "  center_y = y[:,2] * img_h\n",
        "  coord.append((center_x - (bb_box_width / 2)))\n",
        "  coord.append((center_y - (bb_box_height / 2)))\n",
        "  coord.append((center_x + (bb_box_width / 2)))\n",
        "  coord.append((center_y + (bb_box_height / 2)))\n",
        "  \n",
        "  return coord\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPx1YpCWwG_g",
        "colab_type": "text"
      },
      "source": [
        "<h3>IOU Loss</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkvKonJyNXaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iou_loss(labels,logits):\n",
        "  #Convert the arrays to absolute coordinates\n",
        "  coord_labels = convert_to_coord(labels)\n",
        "  coord_logits = convert_to_coord(logits)\n",
        "  \n",
        "  # Calculate the (y1, x1, y2, x2) coordinates of the intersection of box1 and box2. Calculate its Area.\n",
        "  xi1 = tf.math.maximum((coord_labels[0]), (coord_logits[0]))\n",
        "  yi1 = tf.math.maximum((coord_labels[1]), (coord_logits[1]))\n",
        "  xi2 = tf.math.minimum((coord_labels[2]), (coord_logits[2]))\n",
        "  yi2 = tf.math.minimum((coord_labels[3]), (coord_logits[3]))\n",
        "  inter_area = tf.math.maximum((yi2-yi1), 0) * tf.math.maximum((xi2-xi1), 0)\n",
        "  \n",
        "  # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)\n",
        "  box1_area = (coord_labels[3] - coord_labels[1])*(coord_labels[2]- coord_labels[0])\n",
        "  box2_area = (coord_logits[3] - coord_logits[1])*(coord_logits[2]- coord_logits[0])\n",
        "  union_area = (box1_area + box2_area) - inter_area\n",
        "  # compute the IoU\n",
        "  \n",
        "  iou =inter_area / union_area\n",
        "\n",
        "  return iou\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_-8L9oYmr54",
        "colab_type": "text"
      },
      "source": [
        "<h3>Feature Columns</h3>\n",
        "The feature columns is an intermediaries between raw data and Estimators.<br/>\n",
        "Feature columns bridge raw data with the data your model needs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T-N-ccCdy5AU",
        "colab": {}
      },
      "source": [
        "def get_feature_columns():\n",
        "  feature_columns = {'images': tf.feature_column.numeric_column('images', (28, 28, 1))}\n",
        "  return feature_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32tEiXzZu9jq",
        "colab_type": "text"
      },
      "source": [
        "<h3>Base Model</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOSsq5V7RoiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def base_model(input,batch_size, input_shape):\n",
        "  \n",
        "  #print(input.shape)\n",
        "  #print(batch_size)\n",
        "  \n",
        "  \n",
        "  #Conv Layer - 1\n",
        "  x = tf.keras.layers.Conv2D(16,(5,5), input_shape = input_shape , name=\"Conv_1\",\n",
        "                             use_bias=True, kernel_initializer='glorot_uniform', \n",
        "                             bias_initializer='zeros')(input)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.ReLU()(x)\n",
        "  x= tf.keras.layers.MaxPooling2D()(x)\n",
        "  \n",
        "  x = tf.keras.layers.Conv2D(16,(5,5), input_shape = input_shape , name=\"Conv_2\",\n",
        "                             use_bias=True, kernel_initializer='glorot_uniform', \n",
        "                             bias_initializer='zeros')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.ReLU()(x)\n",
        "  \n",
        "  #print(x)\n",
        "  \n",
        "  #Flatten it out\n",
        "  x = tf.keras.layers.Flatten(name=\"Flatten_1\")(x)\n",
        "  #print(\"After flatten\")\n",
        "  #print(x)\n",
        "  \n",
        "  #Dense layer\n",
        "  x = tf.keras.layers.Dense(1024, activation=\"sigmoid\", name=\"Dense1\")(x)\n",
        "  #print(x)\n",
        "  x = tf.keras.layers.Dense(5, activation='sigmoid')(x)\n",
        "  #print(x)\n",
        "  # x = tf.keras.layers.Reshape((2*2, (1*5)), name= 'model_final_reshape')(x)\n",
        "  \n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62R63_cNvgL1",
        "colab_type": "text"
      },
      "source": [
        "<h3>Model Function</h3>\n",
        "<ul>\n",
        "  <li>input is features, labels, mode and parameters</li>\n",
        "</ul>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQn0Lq1O2QZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "                                                                                                \n",
        "  feature_columns = list(get_feature_columns().values())\n",
        "  images = tf.feature_column.input_layer(features=features, feature_columns=feature_columns)\n",
        "  images = tf.reshape(images, shape=(-1, 28, 28, 1),name='my_reshape')\n",
        "\n",
        "  print(\"Printing labels in model function\")\n",
        "  print(labels)\n",
        "\n",
        "  # Calculate logits through CNN                                                                                                            \n",
        "  logits = base_model(images,batch_size, input_shape)\n",
        "\n",
        "  # Create the input layers from the features \n",
        "  if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):\n",
        "    global_step = tf.train.get_or_create_global_step()#create default graph\n",
        "    loss=custom_loss(labels,logits) #loss \n",
        "    iou = iou_loss(labels,logits) \n",
        "    tf.summary.scalar('IOU', tf.math.reduce_mean(iou))\n",
        "    tf.summary.scalar('Loss', loss)\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    predictions = {'coordinates': logits}\n",
        "    export_outputs = {'coordinates': tf.estimator.export.PredictOutput(predictions)}\n",
        "    return tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)\n",
        "  \n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "    iou = iou_loss(labels,logits)\n",
        "    eval_metric_ops = {'iou_eval': tf.metrics.mean(iou)}\n",
        "    return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9,beta2=0.999,epsilon=1e-08,use_locking=False,name='Adam')\n",
        "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM_4JOxFzXfJ",
        "colab_type": "text"
      },
      "source": [
        "Preprocess is used in serving_input_fn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0DU9h4vNbao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(image):\n",
        "  image_contents = tf.read_file(image)\n",
        "  img_decoded = tf.image.decode_jpeg(image_contents, channels=img_channels)\n",
        "  image_converted = tf.image.convert_image_dtype(img_decoded, dtype=tf.float32)\n",
        "  img_expanded = tf.expand_dims(image_converted, 0)\n",
        "  img_resize = tf.image.resize_bilinear(img_expanded,(img_h, img_w))\n",
        "  img_squeezed = tf.squeeze(img_resize,0)\n",
        "  img_standardized = tf.image.per_image_standardization(img_squeezed)\n",
        "  return img_standardized\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19wDjXn4uHHc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo6H6KAoHcHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_and_preprocess(filename):\n",
        "    # decode the image file starting from the filename\n",
        "    image_contents = tf.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(image_contents, channels=img_channels)\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
        "    image= tf.reshape(image, (-1, img_w, img_h, img_channels))\n",
        "    image = tf.image.per_image_standardization(image)\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ81JZ8XIFsq",
        "colab_type": "code",
        "outputId": "774af0fa-941a-49e5-d151-92f307b01d93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "read_and_preprocess('https://drive.google.com/open?id=1a13hnMqYvoqOUMczCdhAfBbkqVm9GzFa')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'per_image_standardization_4:0' shape=(?, 28, 28, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XhDcp5qPoHj",
        "colab_type": "code",
        "outputId": "6788a0e1-d939-4c82-ee45-126a883a1ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "receiver_tensor = {'image_bytes': tf.placeholder(dtype=tf.string, shape=[1])}\n",
        "print(receiver_tensor['image_bytes'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Placeholder_11:0\", shape=(1,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OrzSWWW40uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "receiver_tensor = {'images': tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.string)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OirYTLwKztx2",
        "colab_type": "text"
      },
      "source": [
        "<h3>Serving function</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-09B-yuUMdeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def serving_input_fn():\n",
        "  receiver_tensor = {'images': tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.string)}\n",
        "  features = {'images': tf.map_fn(preprocess, receiver_tensor['images'])}\n",
        "  return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOebbBuhzyg-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCRJa3dH3yud",
        "colab_type": "code",
        "outputId": "405c5f25-453f-49f1-c387-0eb5008db611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "run_config = tf.estimator.RunConfig(model_dir='./Graph', save_summary_steps=10, save_checkpoints_secs = 300, keep_checkpoint_max = 5)\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)\n",
        "\n",
        "# There is another Exporter named FinalExporter which exports the serving graph and checkpoints at the end.\n",
        "\n",
        "exporter = tf.estimator.LatestExporter(\n",
        "  name='Serve',\n",
        "  serving_input_receiver_fn=serving_input_fn,\n",
        "  assets_extra=None,\n",
        "  as_text=False,\n",
        "  exports_to_keep=5)\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(input_fn= lambda:input_fn(files_pattern, batch_size, mode=tf.estimator.ModeKeys.TRAIN))\n",
        "\n",
        "eval_spec = tf.estimator.EvalSpec(lambda: input_fn(files_pattern,  batch_size , mode=tf.estimator.ModeKeys.EVAL), exporters=exporter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0707 16:58:28.287952 139952384415616 model_fn.py:630] Estimator's model_fn (<function model_fn at 0x7f48d0008ea0>) includes params argument, but params are not passed to Estimator.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjod791FBBpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer.add_graph(tf.get_default_graph())\n",
        "writer.flush()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMHahF8sOT_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0E2p9UzzO2C",
        "colab_type": "code",
        "outputId": "84fef5b8-cd39-495a-ac8d-fad63e62b8a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "estimator.train(lambda: input_fn(files_pattern,  batch_size , mode=tf.estimator.ModeKeys.TRAIN), steps=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "Printing labels in model function\n",
            "Tensor(\"IteratorGetNext:1\", shape=(?, 5), dtype=float32, device=/device:CPU:0)\n",
            "Printing labels\n",
            "Tensor(\"IteratorGetNext:1\", shape=(?, 5), dtype=float32, device=/device:CPU:0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0707 16:58:33.496477 139952384415616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0707 16:58:33.591876 139952384415616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f48d0a67898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHOVUP2iEw26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.evaluate(lambda: input_fn(files_pattern,  batch_size , mode=tf.estimator.ModeKeys.EVAL), steps=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_0aCp8F0SJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = estimator.predict(lambda: input_fn(test_pattern,  1 , mode=tf.estimator.ModeKeys.PREDICT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpGdLEZC0xyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_bb(preds):\n",
        "  height_image = 28\n",
        "  width_image = 28\n",
        "  bb_box_width = preds[3] * width_image\n",
        "  bb_box_height = preds[4] * height_image\n",
        "  center_x = preds[1] * width_image\n",
        "  center_y = preds[2] * height_image\n",
        "  x_min = (center_x - (bb_box_width / 2))\n",
        "  y_min = (center_y - (bb_box_height / 2))\n",
        "  print(x_min)\n",
        "  print(y_min)\n",
        "  print(bb_box_width)\n",
        "  print(bb_box_height)\n",
        "  \n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    sess.graph._unsafe_unfinalize()\n",
        "\n",
        "  dataset = tf.data.TFRecordDataset(['/gdrive/My Drive/indic2019/Test/1.tfrecord'])\n",
        "  dataset = dataset.map(extract_fn)\n",
        "  image, label = dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.graph._unsafe_unfinalize()\n",
        "    img = sess.run(image)\n",
        "  img = img[:,:,0]\n",
        "  print(img.shape)\n",
        "  fig,ax = plt.subplots(1)\n",
        "  ax.imshow(img)\n",
        "  rect = patches.Rectangle((x_min,y_min),bb_box_width,bb_box_height,linewidth=1,edgecolor='r',facecolor='none')\n",
        "  ax.add_patch(rect)\n",
        "  plt.show()\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    sess.graph._unsafe_unfinalize()\n",
        "\n",
        "  dataset = tf.data.TFRecordDataset(['/gdrive/My Drive/indic2019/Test/2.tfrecord'])\n",
        "  dataset = dataset.map(extract_fn)\n",
        "  image, label = dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.graph._unsafe_unfinalize()\n",
        "    img = sess.run(image)\n",
        "  img = img[:,:,0]\n",
        "  print(img.shape)\n",
        "  fig,ax = plt.subplots(1)\n",
        "  ax.imshow(img)\n",
        "  rect = patches.Rectangle((x_min,y_min),bb_box_width,bb_box_height,linewidth=1,edgecolor='r',facecolor='none')\n",
        "  ax.add_patch(rect)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxSTyMPbRWRQ",
        "colab_type": "code",
        "outputId": "d56388c3-bbc1-4f06-bdd5-05aaa81b2f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "count = 0\n",
        "for it in l:\n",
        "  preds = it['coordinates']\n",
        "  if count == 0:\n",
        "    break\n",
        "print(preds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ./Graph/model.ckpt-225\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "[0.995943   0.5148804  0.536006   0.86078393 0.8441564 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyop5bqoDhHG",
        "colab_type": "code",
        "outputId": "602f94d5-e259-4dc5-ddf0-b09cc47afc4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "plot_bb(preds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3656766414642334\n",
            "3.1899778842926025\n",
            "24.10195016860962\n",
            "23.6363787651062\n",
            "(28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGP1JREFUeJzt3X103GWVB/DvnUnS0JA2JPSNUqjF\nuoiAhc2psICiqFtQtyjCAtrtsixlXTkrK+K64EHUVVlXRDzHRatUQRBcV1lRUYSq2/UVCostyHu3\n2PeWpGnThjTJzN0/MngC5Pe9SSaZmfp8P+f0NJk7z8yT3/zuvN3nxdwdIpKeXLU7ICLVoeQXSZSS\nXyRRSn6RRCn5RRKl5BdJlJJfJFFKfpFEKflFElVXyTtrbc35nDnZdzkQDDbMIfsKBRhtmydtAaAY\ntGfRIm0JNPCbDv9uC/rupHfsmA225QaC14eoPTvu0WPSH9x3dFwGPLt9znjbSUG8z/mDWh+cFex8\njc5FZsvGAXR1FkZ0A2Ulv5ktAnA9gDyAr7j7Nez6c+bU4e67Ds6MdwR9brTsA7qrWE/bTs3103gv\nOVEAIE9Ohl7P07aH5As03hk8e9SHSZJ93BqDk7g/yN6uYkPQnv/tzbm+zNjUHD8umwuTaLwhSLCO\n4uTMWKPx82F+/XM0vn6AH5eZ+X003l3MPm57fexpecHbtoz4umN+229meQBfAHA6gKMAnGdmR431\n9kSkssr5zL8QwFPuvs7d+wDcDmDx+HRLRCZaOck/G8CGIb9vLF32Ama2zMxWm9nqjo7o07GIVMqE\nf9vv7svdvd3d29vaVFwQqRXlZOMmAHOG/H5o6TIR2Q+Uk/z3A5hvZi8zswYA5wK4c3y6JSITbcw1\nBXcfMLNLANyNwVLfCnd/hLUZcGBHIfv5ptF46YeVtCaylAfwmnSzDdC2PUE5rRD0LSr7dpMyZyHo\nW3OOfw/TSkp1AH9MAF6m3EXKXSMR1eqn5fdmxopBnT4q5a3rm07j5959Do2vXHxtZiwam8HGP0Rj\nH4Yqq87v7ncBuKuc2xCR6tA3cCKJUvKLJErJL5IoJb9IopT8IolS8oskqqLz+QeQQ0fxgMz4TFKX\nBYACqc1G8/kj0bTcllx2vbzZ+HNof1B7/VnvYTR+ygHraZzV4huDw5I3foXuIo/vC47bZHLcuoPp\nwtPyfFpt9Jj3s/ETwXFpcD7+oSXfQ+ONO/hx2VbIzoNyjGYtAL3yiyRKyS+SKCW/SKKU/CKJUvKL\nJErJL5Koipb66lBEW46Xb8YqKnFEK6JGy0iz1Va7aUugEDzHHjlp5CuuDidP/vTeYIZntHpvNNW5\nOVgFl5VQJwfTsKPHLCoVtuSyV9BlpdvB2+alutb8Hhrvm8KP217P7nt0Ls4g9x21HUqv/CKJUvKL\nJErJL5IoJb9IopT8Iomq6Lf9E2X+SdvQsJEv/ilSSetwGb/CB8Z2u8/NrsO6X/LFQ0fqjyL5GzYW\nsPaZl2wW9ALllvqilYWZaPZZNKNwGilZAbzUV4hKfWXOhoyOG/vbyp2JWU6prznYJDQq9UXn0/n/\n+Q80/qV3LM+MsWP6hnlP0NsdjZpKfjZlFwB6yAGPp+TyJagncifczgLfQTjabbYn+NsakX0iR8cl\nSsAdhSYab8vxqa0Mm+4L8CXJAaC+jCfkrcEOwN3FRhoPt3yfzp+w9xaz7/9VDdtp212k7WieUPWZ\nXyRRSn6RRCn5RRKl5BdJlJJfJFFKfpFEKflFElVWnd/M1mNwOnsBwIC7t7PrO3gdMqpRTstn1+qb\ng7nhUR2fDZQBgGYS7+ZlenSR5coBYHIwiCca/1Bv2R1gMQDoCpaQjmrp0XHvi7YfL0NTsP04E61D\nUAyWYz+hkY+fOOBxPk5gysm9mbFofEMzGbMymvn84zHI5/Xu/uw43I6IVJDe9oskqtzkdwA/NrMH\nzGzZeHRIRCqj3Lf9J7v7JjObDuAeM3vM3VcNvULpSWEZAMyazT8niUjllPXK7+6bSv9vB3AHgIXD\nXGe5u7e7e3tLqz5liNSKMWejmTWZWfPzPwN4M4CHx6tjIjKxynnbPwPAHTa4y2sdgG+4+4/GpVci\nMuHGnPzuvg7Aq0d3Z45WMoc7WkO+q5jd3a5gYYe2oJbeEy3eQOqn/Yi2qeb3HfUtqvuyOftdwbz0\n2cH689HYi2hRi3qyVkFngfeN1bOBuG/sviO5YHzED3r4OgfNv+ftez37Ma23YKt68nePvMqvUp9I\nspT8IolS8oskSskvkiglv0iilPwiiaro0t0FGLqL2c83TTleHuknbaOluaOppdES13xLZd7vaXm+\nLXlUyoumBB9etzv7toO+RaW6YjCd+JA6Pq32QMv+254t8uOyeYD/3X1BiXWKZZdQe4O/O9o+/IO/\nPYvGe07lx4WV6zqCx5vtVxBtVT+UXvlFEqXkF0mUkl8kUUp+kUQp+UUSpeQXSZSSXyRRFa3z5+Fo\nJrX8nqCmzJaJjqaHtuazl0oGgBbj4wTyxdFMlnyhqG+NwRLUzTned1bJZ3vUA3FN+cRJfOnuMx57\nJ413r5idGcvzUjp2zeOvTcHpgo9dcEtmbE59B217x67jabz11gNp/PJP3kHj7DFtCs7FJrLseG4U\nk3r1yi+SKCW/SKKU/CKJUvKLJErJL5IoJb9IopT8Iomq+Hz+XWSJ7Gg7abZVdTT/OtrmuieY383m\nf0fLPEfbXEdz4ncU+HP0+oGpmbH59bto28vOXkLjew6bTOOvueJ+Gv/zj2Zv5dAUjEG49Hfn0njX\nw200/ul/eVdmrO2uJ2jbvSceQeOfuP5LNN6Wi9ZwyJ6Tz9aOAIDdPikzVhjF67le+UUSpeQXSZSS\nXyRRSn6RRCn5RRKl5BdJlJJfJFFhnd/MVgB4K4Dt7n506bJWAN8EMBfAegDnuPvOkdwhm28cbbPd\nTOYxR/YF6/JH2z0zPcXsuisQb9HdUShvG2zmrKsvp/F9V3bR+J6N/Pbv/1g7jd9zxAmZse6j+bz1\nAx/j58Oha/j50N+U/dq2422voG2btvGxF9/rOo7GF01dw2+fzNlvDM7zBmSPG7Fxns//NQCLXnTZ\nhwCsdPf5AFaWfheR/UiY/O6+CkDniy5eDOCm0s83AThznPslIhNsrJ/5Z7j7ltLPWwHMGKf+iEiF\nlP2Fn7s7kP1Bw8yWmdlqM1u9s5OPgReRyhlr8m8zs1kAUPp/e9YV3X25u7e7e/tBrSouiNSKsWbj\nnQCWln5eCuC749MdEamUMPnN7DYAvwLwJ2a20cwuBHANgDeZ2ZMA3lj6XUT2I2EB2d3PywidNpY7\nZPuHdxf5+vb9ll2rzwf1zbzxeG8wxoDNsY7m60d96wz+7g39fN76FT/6y8yYt/O+ffTIe2j8U/97\nDo0j2HOg+5XZNevzjruPtj3hlKdofHq+m8bZYxatjf/PT7+Dxh/6xwU0/rN52eMbAOD+T9yQGbtv\nH/9ujK0fMZrRKvoQLpIoJb9IopT8IolS8oskSskvkiglv0iiKrp0dw6ORlIWi7ai7ifTcqOtptk0\nSCBe+rveeXsmWoqZ/V0A0Bj0bd4d2WWrv/rinbTtYfUvnrP1QtctuZHGJy/l05WLnv36Ek11ZufK\n4G3zwlYfso9rNIX7w/O+T+OrPn8kjd/3rmNo/NX/+veZsavem721+Al4Jiwdj5Re+UUSpeQXSZSS\nXyRRSn6RRCn5RRKl5BdJlJJfJFEV36KbLUMd1XU7CweStrwWHtVGe4r1NM5q9dF9R3X8aHzD3Dq+\nzXauL/u49QfLfvcFfWvN76HxECmnR493PxkjMBjnfWdjDKYEYwyi8+VPJ/8fjX/z7NfR+L452WMz\nLv/h+Zmxs/EgPRfZlPkX0yu/SKKU/CKJUvKLJErJL5IoJb9IopT8IolS8oskqrJ1fs+hszA5M86W\nJAaAllxPZizavru/zOe5erIeQGtQp9/nvG+9QS2+OxiD8OQF2XXfz32VL0F9zbIVNN4W1MOjefFR\nrZ5pyfPltQvBtPbOYvZjlguWcn95HX9Mo+XaGztoGKcsejQztureY2nbKZb9mOQx8i3x9Movkigl\nv0iilPwiiVLyiyRKyS+SKCW/SKKU/CKJCuv8ZrYCwFsBbHf3o0uXXQ3gIgA7Sle7wt3vim5rkhUw\nr353ZryryLuzeWBqZuzx/um0LRsjAAC9zmvph9ftzIxFc6h3+yQaj9YDqA/GP9zw+pszY+8pLKVt\nv7B4MY1vPo1vDz5pZ7D1+VldmbHnevkxb27itfaDJ/PHdE9/9viHZ3dlrw0BAP1bs8ejAMDBDwZ7\nBsyhYSyZ9ovM2L0zX0nbXvpk9pbsG3q/zu94iJG88n8NwKJhLr/O3ReU/oWJLyK1JUx+d18FgG/r\nIiL7nXI+819iZmvMbIWZHTRuPRKRihhr8t8A4AgACwBsAXBt1hXNbJmZrTaz1R2dIx93LCITa0zJ\n7+7b3L3g7kUAXwawkFx3ubu3u3t7W6uKCyK1YkzZaGazhvz6dgAPj093RKRSRlLquw3AqQAONrON\nAD4C4FQzWwDAAawHcPEE9lFEJkCY/O5+3jAX803bs24LfA52b7AOO6vF54J5zK15XhOO9npn6wGw\n8QdAXMePllrfNDBlzLf/mTfcTtsuOH0zjV+/4/U0ftRk3v7GdSdlxvry/PTb+xwfH1Es8jeux0zP\n7ttHXv492rYhmK/f/Bd8DMLT/dNonDnn+NU0/syGgzNjfX0jX6JDH8JFEqXkF0mUkl8kUUp+kUQp\n+UUSpeQXSVTFt+jeRZah7iLLegN8u+i2YCvpqJT3+wE+PWEKWZ472sa6p8hLVtF20BFW6ptZlz2l\nFgC2Bsf8/NZf0/iM/HM0fsKr1mXGoqnODWS5dADoKDbReFtub2Zsco6XX6Py7dY+Ho+W9mbn+lum\nPkTb5naRtC1oi24RCSj5RRKl5BdJlJJfJFFKfpFEKflFEqXkF0lURev8hsHlu7PMrec16c5C45jv\ne2+wNHc0hXN7oTkzNjNYWrveBmi8q3gAjbfls+vVALBpoCUzdkzDdtq2s5i9vDUQ963H+Tba2wvZ\nS2Q3lbn9N6vjA8AmMnajJTimk4O+RfZGYzvIORONd8kNjLyWT29nXG5FRPY7Sn6RRCn5RRKl5BdJ\nlJJfJFFKfpFEKflFElXROj/At7PuJnP9ASBHaqPRnPjdRT5GoDnH56WHy28T717JtzXI7+JLlh++\ngC+PfdGc/8mMdTt/iDsKfE48W8cAAHYE7RvJvPlC8NqTD5Zjj7ZVZ7X0SLSWAF8IPh5H0ELOtx92\nH5sZOxNr0NCVnUM5PqTkhdcd+VVF5I+Jkl8kUUp+kUQp+UUSpeQXSZSSXyRRSn6RRIV1fjObA+Bm\nADMwuMv2cne/3sxaAXwTwFwA6wGc4+472W05+BztovPnIlbnj+Z+94PX0r+1cyGN//SW7PhAsMzA\nQSd20PiVr/shjX/gJ+fS+Jc/+Y7M2JJ/51tRz2/YSuPR+vaTweOdZG569JhFdXp2PgB8P4XoXOsL\nzpdoS/hIPen7V+89NTN2Fb6PnnnZaygUJ418D4iRvPIPALjM3Y8CcAKA95rZUQA+BGClu88HsLL0\nu4jsJ8Lkd/ct7v5g6eduAI8CmA1gMYCbSle7CcCZE9VJERl/o/rMb2ZzARwH4DcAZrj7llJoKwY/\nFojIfmLEyW9mBwL4NoBL3X330Ji7OzD84HozW2Zmq81sdVdneZ+TRGT8jCj5zaweg4l/q7t/p3Tx\nNjObVYrPAjDsSpHuvtzd2929vaVVxQWRWhFmo5kZgBsBPOrunx0SuhPA0tLPSwF8d/y7JyITZSRT\nek8CsATAWjN7fu/gKwBcA+A/zOxCAM8AOCe6IQNQT0okXUW+ZPG0fPcIuju8JuNLTP/3Cl7qW/S3\nv8yMHdHIl8eeXrebxqflefyrb/oKjV980LszY1/58Ntp22989loa31rgS1BHJTOmELTdG23hHSy3\nXo5oCnfReN/ZsuEA0E+mWhen8nm5Fy3MnsJ9QxPfLn6oMPnd/edAZkH2tBHfk4jUFH0IF0mUkl8k\nUUp+kUQp+UUSpeQXSZSSXyRRFV26ewA57CC1/MPr6IxgNJJpkC3BlsqXH8erkqf/5Oc0fuzkDZmx\nufXP0rbhEtPBsuP1wd/2rdcsz4yd/+D7adt3XnU5jX/mqhtovC1Y8nw3me/MlvUGgN5gKfe9zrcX\nb8llL7DdQbYOB+LpwtF9R9uys3EEjRv4bR99QPa5eECOj2cZSq/8IolS8oskSskvkiglv0iilPwi\niVLyiyRKyS+SqIrW+XNwWt/cXGim7Q8h8/nfP/dE2nbzB19J4xc23ULjrG4b1Xybg22uIzuC4zKn\nriszdsvfXUfbXvyRS2n8b779Hhp/3xl30fgxjdk16X7ny2N3Fw+g8WhbdaYYvO51B1u6R2sRROsB\nXLw6ew2GSQv4eJcdA1MyYwPBMR1Kr/wiiVLyiyRKyS+SKCW/SKKU/CKJUvKLJErJL5Koitb5Hby2\nG9XDL3ps+NroL/FpYOWhtO2edbzuGm0X3Ub61hLUm/eWOZ8/2q9g60D2OIConn37x/+Nxt/4nQ/Q\n+K2fOp3GF176QGbs1U3ZYwAAYP4kvn14tGcAW0chmm/fW+RjN5qCNRYu+a8LaNwOyT6flrz8Ptp2\nbsOOzFhDML5gKL3yiyRKyS+SKCW/SKKU/CKJUvKLJErJL5IoJb9IosI6v5nNAXAzgBkYLNUvd/fr\nzexqABcBeL7oeIW708ndOTiaybriGwZaaF92rpqZGdty12G07cwtfB324mv58yCrC3cF8877gjnW\n0br+Lfns9ecBXsuPxi+s7ZtO4596y200/tRp2Y8JANz29ez9Eh5YfzxtW7igg8ZfcdB2Gl86/ReZ\nsVXdR9K2d9zN14eo28uP64I3P0Hjb522JjM2rW43bcvGhfBevdBIBvkMALjM3R80s2YAD5jZPaXY\nde7+mVHcn4jUiDD53X0LgC2ln7vN7FEAsye6YyIysUb1md/M5gI4DsBvShddYmZrzGyFmR2U0WaZ\nma02s9Vdnfytt4hUzoiT38wOBPBtAJe6+24ANwA4AsACDL4zuHa4du6+3N3b3b29pVXfL4rUihFl\no5nVYzDxb3X37wCAu29z94K7FwF8GcDCieumiIy3MPnNzADcCOBRd//skMtnDbna2wE8PP7dE5GJ\nMpJv+08CsATAWjN7qHTZFQDOM7MFGCz/rQdwcXRDBeTQRZZErkeBtp/5ho3DBz4BHLyWb0181ufu\npvF5DbxsxKYid5FtxwFgWp6XblqMl/KiJa5zyP4uZevAsF/F/EFrfg+NTwmmWb+m6SkaP/7i9Zmx\nQ+p20baXPX02jf/6Z6+i8YeePTozduxZv6Nt1y75PI1vHuBTeh/vb6Nxhm0tDvDzrTCKr/FG8m3/\nzzF8+ZAv2C4iNU3fwIkkSskvkiglv0iilPwiiVLyiyRKyS+SqIou3V2HIqaRGuaG4lTaftHMRzJj\nH//icto22u452gabieqyUe21t8in9EbbQbMpvTODWnq4hHUw3bi3jG201+7j88OunPsDGu86bOzj\nKzoKB9K2v+qdROO9ztuzsReR7WWci6OhV36RRCn5RRJV0bf9E2XXIY045WXrqt0NkQnXM5t/DBuN\nP4rk/+I9r8OfTX6SXif6zB8td8U0GZ9XkDP++S8au1/OZ/5IuZ/5+52fQuwz/9Zg2bbZdTtpvJw5\nFdFn/mhOQ3RcyvnMH932eNHbfpFEKflFEqXkF0mUufPtocf1zsx2AHhmyEUHA3i2Yh0YnVrtW632\nC1Dfxmo8+3a4u08byRUrmvwvuXOz1e7eXrUOELXat1rtF6C+jVW1+qa3/SKJUvKLJKrayc8H5FdX\nrfatVvsFqG9jVZW+VfUzv4hUT7Vf+UWkSqqS/Ga2yMweN7OnzOxD1ehDFjNbb2ZrzewhM1td5b6s\nMLPtZvbwkMtazeweM3uy9D9fm7uyfbvazDaVjt1DZnZGlfo2x8x+ama/M7NHzOx9pcureuxIv6py\n3Cr+tt/M8gCeAPAmABsB3A/gPHfnC6lXiJmtB9Du7lWvCZvZawHsAXCzux9duuzTADrd/ZrSE+dB\n7v5PNdK3qwHsqfbOzaUNZWYN3VkawJkA/hpVPHakX+egCsetGq/8CwE85e7r3L0PwO0AFlehHzXP\n3VcB6HzRxYsB3FT6+SYMnjwVl9G3muDuW9z9wdLP3QCe31m6qseO9KsqqpH8swFsGPL7RtTWlt8O\n4Mdm9oCZLat2Z4Yxo7RtOgBsBTCjmp0ZRrhzcyW9aGfpmjl2Y9nxerzpC7+XOtndjwdwOoD3lt7e\n1iQf/MxWS+WaEe3cXCnD7Cz9B9U8dmPd8Xq8VSP5NwGYM+T3Q0uX1QR331T6fzuAO1B7uw9ve36T\n1NL/fJPBCqqlnZuH21kaNXDsamnH62ok//0A5pvZy8ysAcC5AO6sQj9ewsyaSl/EwMyaALwZtbf7\n8J0AlpZ+Xgrgu1XsywvUys7NWTtLo8rHruZ2vHb3iv8DcAYGv/F/GsCV1ehDRr/mAfht6d8j1e4b\ngNsw+DawH4PfjVwIoA3ASgBPArgXQGsN9e3rANYCWIPBRJtVpb6djMG39GsAPFT6d0a1jx3pV1WO\nm0b4iSRKX/iJJErJL5IoJb9IopT8IolS8oskSskvkiglv0iilPwiifp/QiI14WHxvv0AAAAASUVO\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuQGFMF56N_T",
        "colab_type": "text"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_pp5Aqs7Sxi",
        "colab_type": "code",
        "outputId": "f7a19338-03f9-483d-982b-005bf091ff9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "estimator.export_saved_model('saved_model', serving_input_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['coordinates', 'serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Restoring parameters from ./Graph/model.ckpt-225\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: saved_model/temp-b'1559848351'/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'saved_model/1559848351'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTISqgx9exsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To export files from google drive to google ml bucket\n",
        "#!gsutil cp '/gdrive/My Drive/indic2019/Test/*.tfrecord' 'gs://indic2019/indic2019'\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7sgBavrBdHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAA5Z-tOE-zA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}